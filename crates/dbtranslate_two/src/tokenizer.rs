use std::collections::{HashMap, HashSet};
use either::Either;
use crate::tokens::{Token, TokenType};


/// This is the overall struct that contains all of the information about 
/// tokenizing strings. 
#[derive(Debug)]
pub struct Tokenizer {
    single_tokens: HashMap<String, TokenType>,
    bit_strings: Vec<Either<String, (String, String)>>,
    byte_strings: Vec<Either<String, (String, String)>>,
    hex_strings: Vec<Either<String, (String, String)>>,
    identifiers: Vec<Either<String, (String, String)>>,
    identifier_escapes: Vec<String>,
    quotes: Vec<Either<String, (String, String)>>,
    string_escapes: Vec<String>,
    // ... add other fields
}

/// These are the implementation methods that are required for the Tokenizer struct.
impl Tokenizer {
    pub fn new() -> Self {
        let single_tokens = maplit::hashmap! {
            "(".to_string() => TokenType::LParen,
            ")".to_string() => TokenType::RParen,
            "[".to_string() => TokenType::LBracket,
            "]".to_string() => TokenType::RBracket,
            "{".to_string() => TokenType::LBrace,
            "}".to_string() => TokenType::RBrace,
            "&".to_string() => TokenType::Amp,
            "^".to_string() => TokenType::Caret,
            ":".to_string() => TokenType::Colon,
            ",".to_string() => TokenType::Comma,
            ".".to_string() => TokenType::Dot,
            "-".to_string() => TokenType::Dash,
            "=".to_string() => TokenType::Eq,
            ">".to_string() => TokenType::Gt,
            "<".to_string() => TokenType::Lt,
            "%".to_string() => TokenType::Mod,
            "!".to_string() => TokenType::Not,
            "|".to_string() => TokenType::Pipe,
            "+".to_string() => TokenType::Plus,
            ";".to_string() => TokenType::Semicolon,
            "/".to_string() => TokenType::Slash,
            "\\".to_string() => TokenType::Backslash,
            "*".to_string() => TokenType::Star,
            "~".to_string() => TokenType::Tilda,
            "?".to_string() => TokenType::Placeholder,
            "@".to_string() => TokenType::Parameter,
            "'".to_string() => TokenType::Quote,
            "`".to_string() => TokenType::Identifier,
            "\"".to_string() => TokenType::Identifier,
            "#".to_string() => TokenType::Hash,
        };

        let keywords = maplit::hashmap! {
            "{{+".to_string() => TokenType::BlockStart,
            "{{-".to_string() => TokenType::BlockStart,
            "+}}".to_string() => TokenType::BlockEnd,
            "-}}".to_string() => TokenType::BlockEnd,
            "/*+".to_string() => TokenType::Hint,
            "==".to_string() => TokenType::Eq,
            "::".to_string() => TokenType::DColon,
            "||".to_string() => TokenType::DPipe,
            ">=".to_string() => TokenType::Gte,
            "<=".to_string() => TokenType::Lte,
            "<>".to_string() => TokenType::Neq,
            "!=".to_string() => TokenType::Neq,
            "<=>".to_string() => TokenType::NullsafeEq,
            "->".to_string() => TokenType::Arrow,
            "->>".to_string() => TokenType::DArrow,
            "=>".to_string() => TokenType::FArrow,
            "#>".to_string() => TokenType::HashArrow,
            "#>>".to_string() => TokenType::DHashArrow,
            "<->".to_string() => TokenType::LrArrow,
            "&&".to_string() => TokenType::Damp,
            "ALL".to_string() => TokenType::All,
            "ALWAYS".to_string() => TokenType::Always,
            "AND".to_string() => TokenType::And,
            "ANTI".to_string() => TokenType::Anti,
            "ANY".to_string() => TokenType::Any,
            "ASC".to_string() => TokenType::Asc,
            "AS".to_string() => TokenType::Alias,
            "AT TIME ZONE".to_string() => TokenType::AtTimeZone,
            "AUTOINCREMENT".to_string() => TokenType::AutoIncrement,
            "AUTO_INCREMENT".to_string() => TokenType::AutoIncrement,
            "BEGIN".to_string() => TokenType::Begin,
            "BETWEEN".to_string() => TokenType::Between,
            "BOTH".to_string() => TokenType::Both,
            "BUCKET".to_string() => TokenType::Bucket,
            "BY DEFAULT".to_string() => TokenType::ByDefault,
            "CACHE".to_string() => TokenType::Cache,
            "UNCACHE".to_string() => TokenType::Uncache,
            "CASE".to_string() => TokenType::Case,
            "CASCADE".to_string() => TokenType::Cascade,
            "CHARACTER SET".to_string() => TokenType::CharacterSet,
            "CLUSTER BY".to_string() => TokenType::ClusterBy,
            "COLLATE".to_string() => TokenType::Collate,
            "COLUMN".to_string() => TokenType::Column,
            "COMMIT".to_string() => TokenType::Commit,
            "COMPOUND".to_string() => TokenType::Compound,
            "CONSTRAINT".to_string() => TokenType::Constraint,
            "CREATE".to_string() => TokenType::Create,
            "CROSS".to_string() => TokenType::Cross,
            "CUBE".to_string() => TokenType::Cube,
            "CURRENT_DATE".to_string() => TokenType::CurrentDate,
            "CURRENT ROW".to_string() => TokenType::CurrentRow,
            "CURRENT_TIME".to_string() => TokenType::CurrentTime,
            "CURRENT_TIMESTAMP".to_string() => TokenType::CurrentTimestamp,
            "CURRENT_USER".to_string() => TokenType::CurrentUser,
            "DATABASE".to_string() => TokenType::Database,
            "DEFAULT".to_string() => TokenType::Default,
            "DELETE".to_string() => TokenType::Delete,
            "DESC".to_string() => TokenType::Desc,
            "DESCRIBE".to_string() => TokenType::Describe,
            "DISTINCT".to_string() => TokenType::Distinct,
            "DISTINCT FROM".to_string() => TokenType::DistinctFrom,
            "DISTRIBUTE BY".to_string() => TokenType::DistributeBy,
            "DIV".to_string() => TokenType::Div,
            "DROP".to_string() => TokenType::Drop,
            "ELSE".to_string() => TokenType::Else,
            "END".to_string() => TokenType::End,
            "ESCAPE".to_string() => TokenType::Escape,
            "EXCEPT".to_string() => TokenType::Except,
            "EXECUTE".to_string() => TokenType::Execute,
            "EXISTS".to_string() => TokenType::Exists,
            "FALSE".to_string() => TokenType::False,
            "FETCH".to_string() => TokenType::Fetch,
            "FILTER".to_string() => TokenType::Filter,
            "FIRST".to_string() => TokenType::First,
            "FULL".to_string() => TokenType::Full,
            "FUNCTION".to_string() => TokenType::Function,
            "FOLLOWING".to_string() => TokenType::Following,
            "FOR".to_string() => TokenType::For,
            "FOREIGN KEY".to_string() => TokenType::ForeignKey,
            "FORMAT".to_string() => TokenType::Format,
            "FROM".to_string() => TokenType::From,
            "GLOB".to_string() => TokenType::Glob,
            "GROUP BY".to_string() => TokenType::GroupBy,
            "GROUPING SETS".to_string() => TokenType::GroupingSets,
            "HAVING".to_string() => TokenType::Having,
            "IF".to_string() => TokenType::If,
            "ILIKE".to_string() => TokenType::ILike,
            "IGNORE NULLS".to_string() => TokenType::IgnoreNulls,
            "IN".to_string() => TokenType::In,
            "INDEX".to_string() => TokenType::Index,
            "INET".to_string() => TokenType::Inet,
            "INNER".to_string() => TokenType::Inner,
            "INSERT".to_string() => TokenType::Insert,
            "INTERVAL".to_string() => TokenType::Interval,
            "INTERSECT".to_string() => TokenType::Intersect,
            "INTO".to_string() => TokenType::Into,
            "IS".to_string() => TokenType::Is,
            "ISNULL".to_string() => TokenType::IsNull,
            "JOIN".to_string() => TokenType::Join,
            "LATERAL".to_string() => TokenType::Lateral,
            "LAZY".to_string() => TokenType::Lazy,
            "LEADING".to_string() => TokenType::Leading,
            "LEFT".to_string() => TokenType::Left,
            "LIKE".to_string() => TokenType::Like,
            "LIMIT".to_string() => TokenType::Limit,
            "LOAD DATA".to_string() => TokenType::LoadData,
            "LOCAL".to_string() => TokenType::Local,
            "MATERIALIZED".to_string() => TokenType::Materialized,
            "MERGE".to_string() => TokenType::Merge,
            "NATURAL".to_string() => TokenType::Natural,
            "NEXT".to_string() => TokenType::Next,
            "NO ACTION".to_string() => TokenType::NoAction,
            "NOT".to_string() => TokenType::Not,
            "NOTNULL".to_string() => TokenType::NotNull,
            "NULL".to_string() => TokenType::Null,
            "NULLS FIRST".to_string() => TokenType::NullsFirst,
            "NULLS LAST".to_string() => TokenType::NullsLast,
            "OBJECT".to_string() => TokenType::Object,
            "OFFSET".to_string() => TokenType::Offset,
            "ON".to_string() => TokenType::On,
            "ONLY".to_string() => TokenType::Only,
            "OPTIONS".to_string() => TokenType::Options,
            "OR".to_string() => TokenType::Or,
            "ORDER BY".to_string() => TokenType::OrderBy,
            "ORDINALITY".to_string() => TokenType::Ordinality,
            "OUTER".to_string() => TokenType::Outer,
            "OUT OF".to_string() => TokenType::OutOf,
            "OVER".to_string() => TokenType::Over,
            "OVERLAPS".to_string() => TokenType::Overlaps,
            "OVERWRITE".to_string() => TokenType::Overwrite,
            "PARTITION".to_string() => TokenType::Partition,
            "PARTITION BY".to_string() => TokenType::PartitionBy,
            "PARTITIONED BY".to_string() => TokenType::PartitionBy,
            "PARTITIONED_BY".to_string() => TokenType::PartitionBy,
            "PERCENT".to_string() => TokenType::Percent,
            "PIVOT".to_string() => TokenType::Pivot,
            "PRAGMA".to_string() => TokenType::Pragma,
            "PRECEDING".to_string() => TokenType::Preceding,
            "PRIMARY KEY".to_string() => TokenType::PrimaryKey,
            "PROCEDURE".to_string() => TokenType::Procedure,
            "QUALIFY".to_string() => TokenType::Qualify,
            "RANGE".to_string() => TokenType::Range,
            "RECURSIVE".to_string() => TokenType::Recursive,
            "REGEXP".to_string() => TokenType::RLike,
            "REPLACE".to_string() => TokenType::Replace,
            "RESPECT NULLS".to_string() => TokenType::RespectNulls,
            "REFERENCES".to_string() => TokenType::References,
            "RIGHT".to_string() => TokenType::Right,
            "RLIKE".to_string() => TokenType::RLike,
            "ROLLBACK".to_string() => TokenType::Rollback,
            "ROLLUP".to_string() => TokenType::Rollup,
            "ROW".to_string() => TokenType::Row,
            "ROWS".to_string() => TokenType::Rows,
            "SCHEMA".to_string() => TokenType::Schema,
            "SEED".to_string() => TokenType::Seed,
            "SELECT".to_string() => TokenType::Select,
            "SEMI".to_string() => TokenType::Semi,
            "SET".to_string() => TokenType::Set,
            "SHOW".to_string() => TokenType::Show,
            "SIMILAR TO".to_string() => TokenType::SimilarTo,
            "SOME".to_string() => TokenType::Some,
            "SORTKEY".to_string() => TokenType::SortKey,
            "SORT BY".to_string() => TokenType::SortBy,
            "TABLE".to_string() => TokenType::Table,
            "TABLESAMPLE".to_string() => TokenType::TableSample,
            "TEMP".to_string() => TokenType::Temporary,
            "TEMPORARY".to_string() => TokenType::Temporary,
            "THEN".to_string() => TokenType::Then,
            "TRUE".to_string() => TokenType::True,
            "TRAILING".to_string() => TokenType::Trailing,
            "UNBOUNDED".to_string() => TokenType::Unbounded,
            "UNION".to_string() => TokenType::Union,
            "UNLOGGED".to_string() => TokenType::Unlogged,
            "UNNEST".to_string() => TokenType::Unnest,
            "UNPIVOT".to_string() => TokenType::Unpivot,
            "UPDATE".to_string() => TokenType::Update,
            "USE".to_string() => TokenType::Use,
            "USING".to_string() => TokenType::Using,
            "UUID".to_string() => TokenType::Uuid,
            "VALUES".to_string() => TokenType::Values,
            "VIEW".to_string() => TokenType::View,
            "VOLATILE".to_string() => TokenType::Volatile,
            "WHEN".to_string() => TokenType::When,
            "WHERE".to_string() => TokenType::Where,
            "WINDOW".to_string() => TokenType::Window,
            "WITH".to_string() => TokenType::With,
            "WITH TIME ZONE".to_string() => TokenType::WithTimeZone,
            "WITH LOCAL TIME ZONE".to_string() => TokenType::WithLocalTimeZone,
            "WITHIN GROUP".to_string() => TokenType::WithinGroup,
            "WITHOUT TIME ZONE".to_string() => TokenType::WithoutTimeZone,
            "APPLY".to_string() => TokenType::Apply,
            "ARRAY".to_string() => TokenType::Array,
            "BIT".to_string() => TokenType::Bit,
            "BOOL".to_string() => TokenType::Boolean,
            "BOOLEAN".to_string() => TokenType::Boolean,
            "BYTE".to_string() => TokenType::Tinyint,
            "TINYINT".to_string() => TokenType::Tinyint,
            "SHORT".to_string() => TokenType::Smallint,
            "SMALLINT".to_string() => TokenType::Smallint,
            "INT2".to_string() => TokenType::Smallint,
            "INTEGER".to_string() => TokenType::Int,
            "INT".to_string() => TokenType::Int,
            "INT4".to_string() => TokenType::Int,
            "LONG".to_string() => TokenType::Bigint,
            "BIGINT".to_string() => TokenType::Bigint,
            "INT8".to_string() => TokenType::Bigint,
            "DEC".to_string() => TokenType::Decimal,
            "DECIMAL".to_string() => TokenType::Decimal,
            "BIGDECIMAL".to_string() => TokenType::Bigdecimal,
            "BIGNUMERIC".to_string() => TokenType::Bigdecimal,
            "MAP".to_string() => TokenType::Map,
            "NULLABLE".to_string() => TokenType::Nullable,
            "NUMBER".to_string() => TokenType::Decimal,
            "NUMERIC".to_string() => TokenType::Decimal,
            "FIXED".to_string() => TokenType::Decimal,
            "REAL".to_string() => TokenType::Float,
            "FLOAT".to_string() => TokenType::Float,
            "FLOAT4".to_string() => TokenType::Float,
            "FLOAT8".to_string() => TokenType::Double,
            "DOUBLE".to_string() => TokenType::Double,
            "DOUBLE PRECISION".to_string() => TokenType::Double,
            "JSON".to_string() => TokenType::Json,
            "CHAR".to_string() => TokenType::Char,
            "CHARACTER".to_string() => TokenType::Char,
            "NCHAR".to_string() => TokenType::Nchar,
            "VARCHAR".to_string() => TokenType::Varchar,
            "VARCHAR2".to_string() => TokenType::Varchar,
            "NVARCHAR".to_string() => TokenType::Nvarchar,
            "NVARCHAR2".to_string() => TokenType::Nvarchar,
            "STR".to_string() => TokenType::Text,
            "STRING".to_string() => TokenType::Text,
            "TEXT".to_string() => TokenType::Text,
            "CLOB".to_string() => TokenType::Text,
            "LONGVARCHAR".to_string() => TokenType::Text,
            "BINARY".to_string() => TokenType::Binary,
            "BLOB".to_string() => TokenType::Varbinary,
            "BYTEA".to_string() => TokenType::Varbinary,
            "VARBINARY".to_string() => TokenType::Varbinary,
            "TIME".to_string() => TokenType::Time,
            "TIMESTAMP".to_string() => TokenType::Timestamp,
            "TIMESTAMPTZ".to_string() => TokenType::Timestamptz,
            "TIMESTAMPLTZ".to_string() => TokenType::Timestampltz,
            "DATE".to_string() => TokenType::Date,
            "DATETIME".to_string() => TokenType::Datetime,
            "UNIQUE".to_string() => TokenType::Unique,
            "STRUCT".to_string() => TokenType::Struct,
            "VARIANT".to_string() => TokenType::Variant,
            "ALTER".to_string() => TokenType::Alter,
            "ALTER AGGREGATE".to_string() => TokenType::Command,
            "ALTER DEFAULT".to_string() => TokenType::Command,
            "ALTER DOMAIN".to_string() => TokenType::Command,
            "ALTER ROLE".to_string() => TokenType::Command,
            "ALTER RULE".to_string() => TokenType::Command,
            "ALTER SEQUENCE".to_string() => TokenType::Command,
            "ALTER TYPE".to_string() => TokenType::Command,
            "ALTER USER".to_string() => TokenType::Command,
            "ALTER VIEW".to_string() => TokenType::Command,
            "ANALYZE".to_string() => TokenType::Command,
            "CALL".to_string() => TokenType::Command,
            "COMMENT".to_string() => TokenType::Comment,
            "COPY".to_string() => TokenType::Command,
            "EXPLAIN".to_string() => TokenType::Command,
            "GRANT".to_string() => TokenType::Command,
            "OPTIMIZE".to_string() => TokenType::Command,
            "PREPARE".to_string() => TokenType::Command,
            "TRUNCATE".to_string() => TokenType::Command,
            "VACUUM".to_string() => TokenType::Command,
        };

        let comments = maplit::hashmap! {
            "--".to_string() => None,
            "/*".to_string() => Some("*/".to_string()),
            "{#".to_string() => Some("#}".to_string()),
        };

        let white_space = maplit::hashmap! {
            " ".to_string() => TokenType::Space,
            "\t".to_string() => TokenType::Space,
            "\n".to_string() => TokenType::Break,
            "\r".to_string() => TokenType::Break,
            "\r\n".to_string() => TokenType::Break,
        };

        let commands: HashSet<TokenType> = [
            TokenType::Command,
            TokenType::Execute,
            TokenType::Fetch,
            TokenType::Show,
        ].iter().cloned().collect();

        let bit_strings = vec![];
        let byte_strings = vec![];
        let hex_strings = vec![];
        let identifiers = vec![Either::Left("\"".to_string())];
        let identifier_escapes = vec!["\"".to_string()];
        let quotes = vec![Either::Left("'".to_string())];
        let string_escapes = vec!["'".to_string()];

        // ... add other initializations

        Tokenizer {
            single_tokens,
            bit_strings,
            byte_strings,
            hex_strings,
            identifiers,
            identifier_escapes,
            quotes,
            string_escapes,
            // ... add other field assignments
        }
    }

    fn delimeter_list_to_dict(
        list: Vec<Either<String, (String, String)>>,
    ) -> HashMap<String, String> {
        let mut dict = HashMap::new();
        for item in list {
            match item {
                Either::Left(s) => {
                    dict.insert(s.clone(), s);
                }
                Either::Right((k, v)) => {
                    dict.insert(k, v);
                }
            }
        }
        dict
    }

    // Add other required methods
}